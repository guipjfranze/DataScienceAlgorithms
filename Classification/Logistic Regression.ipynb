{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, pickle\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]\n",
      "Nrow: 683\n",
      "Ncol: 11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_code</th>\n",
       "      <th>clump_tickness</th>\n",
       "      <th>uniformity_cell_size</th>\n",
       "      <th>uniformity_cell_shape</th>\n",
       "      <th>marginal_adhesion</th>\n",
       "      <th>single_epithelial_cell_size</th>\n",
       "      <th>bare_nuclei</th>\n",
       "      <th>bland_chromatin</th>\n",
       "      <th>normal_nucleoli</th>\n",
       "      <th>mitoses</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_code  clump_tickness  uniformity_cell_size  uniformity_cell_shape  \\\n",
       "0      1000025               5                     1                      1   \n",
       "1      1002945               5                     4                      4   \n",
       "\n",
       "   marginal_adhesion  single_epithelial_cell_size  bare_nuclei  \\\n",
       "0                  1                            2            1   \n",
       "1                  5                            7           10   \n",
       "\n",
       "   bland_chromatin  normal_nucleoli  mitoses  target  \n",
       "0                3                1        1       2  \n",
       "1                3                2        1       2  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Attribute Information\n",
    "\n",
    "# 1. Sample code number: id number \n",
    "# 2. Clump Thickness: 1 - 10 \n",
    "# 3. Uniformity of Cell Size: 1 - 10 \n",
    "# 4. Uniformity of Cell Shape: 1 - 10 \n",
    "# 5. Marginal Adhesion: 1 - 10 \n",
    "# 6. Single Epithelial Cell Size: 1 - 10 \n",
    "# 7. Bare Nuclei: 1 - 10 \n",
    "# 8. Bland Chromatin: 1 - 10 \n",
    "# 9. Normal Nucleoli: 1 - 10 \n",
    "# 10. Mitoses: 1 - 10 \n",
    "# 11. Class: (2 for benign, 4 for malignant)\n",
    "\n",
    "# Specifing header\n",
    "header = ['sample_code','clump_tickness','uniformity_cell_size','uniformity_cell_shape',\n",
    "          'marginal_adhesion','single_epithelial_cell_size','bare_nuclei','bland_chromatin',\n",
    "          'normal_nucleoli','mitoses','target']\n",
    "\n",
    "# Reading the data\n",
    "folderName = '../data/' \n",
    "fileName   = 'breast-cancer-wisconsin.data'\n",
    "model_file_name   = 'decisionTree_model.sav'\n",
    "\n",
    "df = pd.read_csv(folderName+fileName,header=None,names=header)\n",
    "\n",
    "# Printing some info about the data\n",
    "print(\"[INFO]\\nNrow: {}\\nNcol: {}\".format(df.shape[0],df.shape[1]))\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clump_tickness</th>\n",
       "      <th>uniformity_cell_size</th>\n",
       "      <th>uniformity_cell_shape</th>\n",
       "      <th>marginal_adhesion</th>\n",
       "      <th>single_epithelial_cell_size</th>\n",
       "      <th>bare_nuclei</th>\n",
       "      <th>bland_chromatin</th>\n",
       "      <th>normal_nucleoli</th>\n",
       "      <th>mitoses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clump_tickness  uniformity_cell_size  uniformity_cell_shape  \\\n",
       "0               5                     1                      1   \n",
       "1               5                     4                      4   \n",
       "2               3                     1                      1   \n",
       "\n",
       "   marginal_adhesion  single_epithelial_cell_size  bare_nuclei  \\\n",
       "0                  1                            2            1   \n",
       "1                  5                            7           10   \n",
       "2                  1                            2            2   \n",
       "\n",
       "   bland_chromatin  normal_nucleoli  mitoses  \n",
       "0                3                1        1  \n",
       "1                3                2        1  \n",
       "2                3                1        1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the target column\n",
    "target = df.loc[:,df.columns[-1]].values\n",
    "# target = np.where(target==4,1,0)\n",
    "\n",
    "# If necessary, dropping cols\n",
    "cols = df.columns # List of columns in dataframe\n",
    "drop = [cols[-1],cols[0]] # List of columns to drop\n",
    "df.drop(labels=drop,axis=1,inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns to int or float to avoid errors\n",
    "def conv2num(df):\n",
    "    cols = df.columns\n",
    "    for col in cols:\n",
    "        try:\n",
    "            df[str(col)] = df[str(col)].astype(float) # Remember to specify type according to data specification\n",
    "        except Exception as e:\n",
    "            print('Column \\'{}\\' was not converted. Error: \\n'.format(col), e, '\\n')\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "df = conv2num(df) # Converting columns to number to avoid errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "clf = LogisticRegression(penalty='l1',C=0.1)\n",
    "\n",
    "# ----------- CROSS VALIDATION ----------- #\n",
    "# ---------------- K-Fold ---------------- #\n",
    "accuracy_train=[]\n",
    "precision_train=[]\n",
    "recall_train=[]\n",
    "f1_train=[]\n",
    "auc_train=[]\n",
    "\n",
    "accuracy_test=[]\n",
    "precision_test=[]\n",
    "recall_test=[]\n",
    "f1_test=[]\n",
    "auc_test=[]\n",
    "cv = model_selection.KFold(n_splits=10,shuffle=False) # K-fold Cross Validation method\n",
    "for train_index, test_index in cv.split(df.values):\n",
    "    X_train, X_test = df.values[train_index,:], df.values[test_index,:]\n",
    "    y_train, y_test = target[train_index],target[test_index]  \n",
    "    \n",
    "    # Fitting the data into the model\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # FOR TRAINING\n",
    "    predicted_train = clf.predict(X_train)\n",
    "    accuracy_train.append(metrics.accuracy_score(y_train, predicted_train))\n",
    "    precision_train.append(metrics.precision_score(y_train, predicted_train,pos_label=4))\n",
    "    recall_train.append(metrics.recall_score(y_train, predicted_train,pos_label=4))\n",
    "    f1_train.append(metrics.f1_score(y_train, predicted_train,pos_label=4))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_train, predicted_train,pos_label=4)\n",
    "    auc_train.append(metrics.auc(fpr, tpr))\n",
    "    \n",
    "    # FOR TESTING\n",
    "    predicted_test = clf.predict(X_test)\n",
    "    accuracy_test.append(metrics.accuracy_score(y_test, predicted_test))\n",
    "    precision_test.append(metrics.precision_score(y_test, predicted_test,pos_label=4))\n",
    "    recall_test.append(metrics.recall_score(y_test, predicted_test,pos_label=4))\n",
    "    f1_test.append(metrics.f1_score(y_test, predicted_test,pos_label=4))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, predicted_test,pos_label=4)\n",
    "    auc_test.append(metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Train = 0.965837769126877\n",
      "Precision Train = 0.9603238725324819\n",
      "Recall Train = 0.9414534896009424\n",
      "F1-score Train = 0.950777270184123\n",
      "AUC Train = 0.9601816301978834\n",
      "\n",
      "Accuracy Test = 0.965837769126877\n",
      "Precision Test = 0.9603238725324819\n",
      "Recall Test = 0.9414534896009424\n",
      "F1-score Test = 0.950777270184123\n",
      "AUC Test = 0.9601816301978834\n"
     ]
    }
   ],
   "source": [
    "# Calculating the mean values for train and test scores\n",
    "accuracy_train = np.array(accuracy_train).mean()\n",
    "precision_train = np.array(precision_train).mean()\n",
    "recall_train = np.array(recall_train).mean()\n",
    "f1_train = np.array(f1_train).mean()\n",
    "auc_train = np.array(auc_train).mean()\n",
    "\n",
    "accuracy_test = np.array(accuracy_train).mean()\n",
    "precision_test = np.array(precision_train).mean()\n",
    "recall_test = np.array(recall_train).mean()\n",
    "f1_test = np.array(f1_train).mean()\n",
    "auc_test = np.array(auc_train).mean()\n",
    "\n",
    "# Displaying the results\n",
    "print('Accuracy Train = {}'.format(accuracy_train))\n",
    "print('Precision Train = {}'.format(precision_train))\n",
    "print('Recall Train = {}'.format(recall_train))\n",
    "print('F1-score Train = {}'.format(f1_train))\n",
    "print('AUC Train = {}'.format(auc_train))\n",
    "print()\n",
    "print('Accuracy Test = {}'.format(accuracy_test))\n",
    "print('Precision Test = {}'.format(precision_test))\n",
    "print('Recall Test = {}'.format(recall_test))\n",
    "print('F1-score Test = {}'.format(f1_test))\n",
    "print('AUC Test = {}'.format(auc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hold-Out Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "clf = LogisticRegression(penalty='l1',C=0.1)\n",
    "\n",
    "accuracy_train=[]\n",
    "precision_train=[]\n",
    "recall_train=[]\n",
    "f1_train=[]\n",
    "auc_train=[]\n",
    "\n",
    "accuracy_test=[]\n",
    "precision_test=[]\n",
    "recall_test=[]\n",
    "f1_test=[]\n",
    "auc_test=[]\n",
    "\n",
    "# ----------- CROSS VALIDATION ----------- #\n",
    "# -------------- Hold-Out -------------- # When I have too many data\n",
    "X_train, X_test, y_train, y_test  = model_selection.train_test_split(df.values, \n",
    "                                                                     target, \n",
    "                                                                     test_size=0.5,\n",
    "                                                                     random_state=None,\n",
    "                                                                     shuffle=False)\n",
    "\n",
    "# Fitting the data into the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# FOR TRAINING\n",
    "predicted_train = clf.predict(X_train)\n",
    "accuracy_train.append(metrics.accuracy_score(y_train, predicted_train))\n",
    "precision_train.append(metrics.precision_score(y_train, predicted_train,pos_label=4))\n",
    "recall_train.append(metrics.recall_score(y_train, predicted_train,pos_label=4))\n",
    "f1_train.append(metrics.f1_score(y_train, predicted_train,pos_label=4))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, predicted_train,pos_label=4)\n",
    "auc_train.append(metrics.auc(fpr, tpr))\n",
    "\n",
    "# FOR TESTING\n",
    "predicted_test = clf.predict(X_test)\n",
    "accuracy_test.append(metrics.accuracy_score(y_test, predicted_test))\n",
    "precision_test.append(metrics.precision_score(y_test, predicted_test,pos_label=4))\n",
    "recall_test.append(metrics.recall_score(y_test, predicted_test,pos_label=4))\n",
    "f1_test.append(metrics.f1_score(y_test, predicted_test,pos_label=4))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, predicted_test,pos_label=4)\n",
    "auc_test.append(metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Train = 0.9589442815249267\n",
      "Precision Train = 0.9444444444444444\n",
      "Recall Train = 0.9683544303797469\n",
      "F1-score Train = 0.9562499999999999\n",
      "AUC Train = 0.9595870512554472\n",
      "\n",
      "Accuracy Test = 0.9589442815249267\n",
      "Precision Test = 0.9444444444444444\n",
      "Recall Test = 0.9683544303797469\n",
      "F1-score Test = 0.9562499999999999\n",
      "AUC Test = 0.9595870512554472\n"
     ]
    }
   ],
   "source": [
    "# Calculating the mean values for train and test scores\n",
    "accuracy_train = np.array(accuracy_train).mean()\n",
    "precision_train = np.array(precision_train).mean()\n",
    "recall_train = np.array(recall_train).mean()\n",
    "f1_train = np.array(f1_train).mean()\n",
    "auc_train = np.array(auc_train).mean()\n",
    "\n",
    "accuracy_test = np.array(accuracy_train).mean()\n",
    "precision_test = np.array(precision_train).mean()\n",
    "recall_test = np.array(recall_train).mean()\n",
    "f1_test = np.array(f1_train).mean()\n",
    "auc_test = np.array(auc_train).mean()\n",
    "\n",
    "# Displaying the results\n",
    "print('Accuracy Train = {}'.format(accuracy_train))\n",
    "print('Precision Train = {}'.format(precision_train))\n",
    "print('Recall Train = {}'.format(recall_train))\n",
    "print('F1-score Train = {}'.format(f1_train))\n",
    "print('AUC Train = {}'.format(auc_train))\n",
    "print()\n",
    "print('Accuracy Test = {}'.format(accuracy_test))\n",
    "print('Precision Test = {}'.format(precision_test))\n",
    "print('Recall Test = {}'.format(recall_test))\n",
    "print('F1-score Test = {}'.format(f1_test))\n",
    "print('AUC Test = {}'.format(auc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-One-Out Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the classifier\n",
    "clf = LogisticRegression(penalty='l1',C=0.1)\n",
    "\n",
    "# ----------- CROSS VALIDATION ----------- #\n",
    "# ------------- Leave-One-Out ------------- #\n",
    "accuracy_train=[]\n",
    "precision_train=[]\n",
    "recall_train=[]\n",
    "f1_train=[]\n",
    "auc_train=[]\n",
    "\n",
    "pred_test=[]\n",
    "accuracy_test=[]\n",
    "precision_test=[]\n",
    "recall_test=[]\n",
    "f1_test=[]\n",
    "auc_test=[]\n",
    "cv = model_selection.LeaveOneOut() # K-fold Cross Validation method\n",
    "for train_index, test_index in cv.split(df.values):\n",
    "    X_train, X_test = df.values[train_index,:], df.values[test_index,:]\n",
    "    y_train, y_test = target[train_index],target[test_index]  \n",
    "    \n",
    "    # Fitting the data into the model\n",
    "    clf.fit(X_train, y_train)\n",
    "        \n",
    "    # FOR TRAINING\n",
    "    predicted_train = clf.predict(X_train)\n",
    "    accuracy_train.append(metrics.accuracy_score(y_train, predicted_train))\n",
    "    precision_train.append(metrics.precision_score(y_train, predicted_train,pos_label=4))\n",
    "    recall_train.append(metrics.recall_score(y_train, predicted_train,pos_label=4))\n",
    "    f1_train.append(metrics.f1_score(y_train, predicted_train,pos_label=4))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_train, predicted_train,pos_label=4)\n",
    "    auc_train.append(metrics.auc(fpr, tpr))\n",
    "\n",
    "    # Storing testing\n",
    "    pred_test.append([y_test, clf.predict(X_test)])\n",
    "\n",
    "# Converting to array\n",
    "pred_test = np.array(pred_test)\n",
    "    \n",
    "# FOR TESTING\n",
    "accuracy_test = metrics.accuracy_score(pred_test[:,0,0], pred_test[:,1,0])\n",
    "precision_test = metrics.precision_score(pred_test[:,0,0], pred_test[:,1,0],pos_label=4)\n",
    "recall_test = metrics.recall_score(pred_test[:,0,0], pred_test[:,1,0],pos_label=4)\n",
    "f1_test = metrics.f1_score(pred_test[:,0,0], pred_test[:,1,0],pos_label=4)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(pred_test[:,0,0], pred_test[:,1,0],pos_label=4)\n",
    "auc_test = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Train = 0.9649553676852596\n",
      "Precision Train = 0.9613779500143748\n",
      "Recall Train = 0.937514501135458\n",
      "F1-score Train = 0.9492958327611514\n",
      "AUC Train = 0.9586204629169693\n",
      "\n",
      "Accuracy Test = 0.9604685212298683\n",
      "Precision Test = 0.9568965517241379\n",
      "Recall Test = 0.9288702928870293\n",
      "F1-score Test = 0.9426751592356688\n",
      "AUC Test = 0.9531738851822533\n"
     ]
    }
   ],
   "source": [
    "# Calculating the mean values for train and test scores\n",
    "accuracy_train = np.array(accuracy_train).mean()\n",
    "precision_train = np.array(precision_train).mean()\n",
    "recall_train = np.array(recall_train).mean()\n",
    "f1_train = np.array(f1_train).mean()\n",
    "auc_train = np.array(auc_train).mean()\n",
    "\n",
    "# Displaying the results\n",
    "print('Accuracy Train = {}'.format(accuracy_train))\n",
    "print('Precision Train = {}'.format(precision_train))\n",
    "print('Recall Train = {}'.format(recall_train))\n",
    "print('F1-score Train = {}'.format(f1_train))\n",
    "print('AUC Train = {}'.format(auc_train))\n",
    "print()\n",
    "print('Accuracy Test = {}'.format(accuracy_test))\n",
    "print('Precision Test = {}'.format(precision_test))\n",
    "print('Recall Test = {}'.format(recall_test))\n",
    "print('F1-score Test = {}'.format(f1_test))\n",
    "print('AUC Test = {}'.format(auc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
